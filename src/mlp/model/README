# MULTILAYER PERCEPTRON : BÂTIR LE MODÈLE

## IMPLÉMENTER LE MULTILAYER PERCEPTRON

### Architecture des couches

La règle la plus importante est de s'assurer qu'on a toujours un **nombre d'inputs de la couche n, égal au nombre de neurones de la couche n-1**, sauf évidemment dans le cadre de l'input layer.

```python
# Exemple d'architecture
Input Layer:    features (30) → Hidden Layer 1 (16 neurones)
Hidden Layer 1: 16 inputs    → Hidden Layer 2 (8 neurones) 
Hidden Layer 2: 8 inputs     → Output Layer (2 neurones pour softmax ou 1 pour sigmoid)
```

### Initialisation des poids et biais

On initialise les **weights au hasard** mais en fixant des **petites valeurs entre -1 et 1** pour garantir une plus grande stabilité mathématique du calcul.

- `n_inputs` sera le nombre de features 
- `n_neurons` le nombre de neurones de la couche
- **Utiliser `np.random.randn()` pour produire des valeurs random** (utiliser une seed pour reproductibilité)

```python
# Initialisation des poids
self.weights = np.random.randn(n_inputs, n_neurons) * 0.1

# Initialisation des biais  
self.biases = np.zeros((1, n_neurons))
```

Pour utiliser le biais, on va créer une **matrice de 0** de la taille du nombre de neurones avec `np.zeros`. Cette fonction attend normalement un seul argument, on met donc **un tuple en argument** afin de spécifier les deux dimensions.

⚠️ **Important :** `self.biases` est donc de forme `[[0.0, 0.0, 0.0...]]` => **2D**, on ne bosse qu'en 2D donc pas de `[0.0...]`.

---

## LES FONCTIONS D'ACTIVATION

Elles permettent de rendre plus interprétables les résultats des fonctions de coût.

### La fonction Sigmoid
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-np.clip(x, -250, 250)))  # Clip pour éviter overflow
```

Elle permet de transformer la sortie lors de **classification binaire**, on l'utilise pour les sorties output layer à **un seul neurone**.
- **Sortie :** valeurs entre 0 et 1
- **Usage :** Output layer pour classification binaire

### La fonction ReLU
```python
def relu(x):
    return np.maximum(0, x)
```

Elle est plus simple et marche pour les **hidden layers**, donne 0 si négatif et conserve la valeur si positif. Cet aspect simple est plus efficace et marche bien pour les couches cachées.
- **Sortie :** 0 ou valeur positive
- **Usage :** Hidden layers principalement

### La fonction SoftMax
```python
def softmax(x):
    # Soustraire le max pour stabilité numérique
    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_values / np.sum(exp_values, axis=1, keepdims=True)
```

La SoftMax est utilisée en sortie de réseau sur **l'output layer** car elle va retourner des **probabilités**, ce qui est plus utile dans une tâche de classification multi-classes. L'obtention de probabilités permet de savoir à quel point le modèle est confiant, ce qu'on ne pourrait pas calculer avec des résultats 0 et 1.

Dans la couche "output layer", elle transforme la sortie de vecteurs de floats de la dernière hidden layer en **vecteurs de probabilités** qui somment à 1.

**NB :** Elle utilise l'exponentielle + la normalisation pour assurer la stabilité mathématique du calcul.

---

## LA LOG LOSS : CATEGORICAL CROSS ENTROPY

L'entropie croisée est particulièrement bien adaptée aux **problèmes de classification multi-classes**.
Elle pénalise fortement les prédictions incorrectes, ce qui aide le modèle à apprendre à être plus précis.

### Architecture modulaire

On implémente une **classe parente abstraite** dont la méthode `calculate` va appeler le calcul de la loss sur chaque échantillon du dataset, ensuite on va en faire la moyenne et retourner le résultat.

Cette architecture apporte de la **modularité** si on souhaite utiliser une autre fonction de loss que la Categorical Cross Entropy (Mean Squared Error ou Binary Cross Entropy par exemple).

```python
class Loss:
    def calculate(self, output, y):
        sample_losses = self.forward(output, y)
        data_loss = np.mean(sample_losses)
        return data_loss
    
    def forward(self, y_pred, y_true):
        raise NotImplementedError
```

### Implémentation Categorical Cross Entropy

```python
class CategoricalCrossentropy(Loss):
    def forward(self, y_pred, y_true):
        samples = len(y_pred)
        
        # Clip pour éviter log(0) = -inf
        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)
        
        # Si y_true est en format sparse (indices)
        if len(y_true.shape) == 1:
            correct_confidences = y_pred_clipped[range(samples), y_true]
        # Si y_true est en format one-hot
        elif len(y_true.shape) == 2:
            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)
            
        negative_log_likelihoods = -np.log(correct_confidences)
        return negative_log_likelihoods
```

**Gestion des valeurs problématiques :**
Puisque le logarithme de 0 est -∞, on doit supprimer les 0 des prédictions, idem pour 1 (ces deux valeurs sont problématiques).

**`np.clip()`** : Cette fonction limite les valeurs d'un tableau NumPy à un intervalle donné `[1e-7, 1-1e-7]`.

### Principe d'indexation avancée

La ligne de code :
```python
correct_confidences = y_pred_clipped[range(samples), y_true]
```

**Exemple :**
Si `y_true` est `[1, 2, 0]` et `y_pred_clipped` est :
```python
[
    [0.1, 0.6, 0.3],  # Échantillon 0: classe correcte 1, proba = 0.6
    [0.3, 0.4, 0.3],  # Échantillon 1: classe correcte 2, proba = 0.3  
    [0.2, 0.5, 0.3]   # Échantillon 2: classe correcte 0, proba = 0.2
]
```

Le `range(samples)` va renvoyer une séquence d'entiers de 0 à `len(samples) - 1`.
La sélection avec l'indexation avancée récupère les valeurs `[0.6, 0.3, 0.2]`, qui sont les probabilités que le modèle a assignées aux **classes correctes** selon `y_true`.

⚠️ **Important :** On ne compare pas les probas des predictions et des truths directement, c'est le **logarithme négatif** qui vient pénaliser les prédictions faibles !

---

## OPTIMISATION DE LA LOSS ⇒ TRAINING DU MODÈLE MLP

### Pipeline d'entraînement

```
Entrée brute → Prédictions brutes → Softmax → Probabilités → Calcul de la Loss → Backpropagation
```

### Différenciation des types de couches

| Type de couche | Rôle | Implémentation |
|----------------|------|----------------|
| **Input layer** | Reçoit les données d'entrée | Souvent juste une DenseLayer(n_inputs, ...) |
| **Hidden layers** | Apprennent les représentations | DenseLayer(..., n_neurons) + ReLU/sigmoid |
| **Output layer** | Donne les prédictions finales | DenseLayer(..., n_classes) + Softmax/sigmoid |

### Boucle d'entraînement

En pseudo-code simplifié :
```python
for epoch in range(n_epochs):
    for batch in batches:
        # Forward pass
        output = mlp.forward(X_batch)
        
        # Calcul de la loss
        loss = loss_function.calculate(output, y_batch)
        
        # Backward pass
        mlp.backward(y_batch)
        
        # Mise à jour des poids
        optimizer.step(mlp.layers)
```

### Concepts clés

- **Une epoch** correspond à un passage complet du dataset de train dans le réseau
- On divise le dataset de training en unités dites **"batches"**
- **Exemple :** Si on a 1000 samples dans le dataset, et un batch_size de 100, alors une epoch = 10 batches de 100 samples

---

## BACKPROPAGATION : LA CHAÎNE MATHÉMATIQUE

### Décomposition des étapes

La difficulté est de suivre et décomposer les étapes classiques de la backpropagation :

1. **Forward pass** → calcul de toutes les sorties (z, a) avec `a` les outputs activés
2. **Backward pass** → on remonte les dérivées avec la chaîne de dérivation

### La chaîne des gradients

Dans un MLP, le calcul direct de `dLoss/dW` n'est pas possible, on doit donc **décomposer le calcul** pour arriver vers `dW` et donc les poids ajustés.

#### Étapes de la chaîne :

```
dLoss/dW = dLoss/da × da/dz × dz/dW
```

Où :
- **`z`** = sortie brute du neurone (avant activation) = `W·X + b`
- **`a`** = sortie activée = `activation(z)`
- **`Loss`** = fonction de coût

#### Calculs détaillés :

**1. Pour la couche de sortie :**
```python
# dLoss/da (gradient de la loss par rapport aux activations)
if using_softmax_with_categorical_crossentropy:
    dLoss_da = y_pred - y_true  # Forme simplifiée !
else:
    dLoss_da = -(y_true / y_pred) + (1 - y_true) / (1 - y_pred)

# da/dz (dérivée de la fonction d'activation)
if activation == 'softmax':
    # Complexe pour softmax générale, mais simplifiée avec categorical crossentropy
    da_dz = 1  # Quand combinée avec categorical crossentropy
elif activation == 'sigmoid':
    da_dz = a * (1 - a)
elif activation == 'relu':
    da_dz = (z > 0).astype(float)

# dz/dW (gradient par rapport aux poids)
dz_dW = inputs  # Les inputs de cette couche

# dz/db (gradient par rapport aux biais)  
dz_db = 1

# Gradient final pour les poids
dLoss_dW = np.dot(inputs.T, dLoss_dz)  # où dLoss_dz = dLoss_da * da_dz

# Gradient final pour les biais
dLoss_db = np.sum(dLoss_dz, axis=0, keepdims=True)
```

**2. Pour les couches cachées (propagation vers l'arrière) :**
```python
# Le gradient vient de la couche suivante
dLoss_da = np.dot(dLoss_dz_next, weights_next.T)

# Puis on applique la même chaîne
da_dz = activation_derivative(z)
dLoss_dz = dLoss_da * da_dz

# Et on calcule les gradients des poids
dLoss_dW = np.dot(inputs.T, dLoss_dz)
dLoss_db = np.sum(dLoss_dz, axis=0, keepdims=True)
```

### Mise à jour des paramètres

```python
# Gradient descent simple
learning_rate = 0.01

weights -= learning_rate * dLoss_dW
biases -= learning_rate * dLoss_db
```

#### Optimiseurs plus avancés :

**Adam Optimizer :**
```python
# Moments exponentiels
m_w = beta1 * m_w + (1 - beta1) * dLoss_dW
v_w = beta2 * v_w + (1 - beta2) * (dLoss_dW ** 2)

# Correction de biais
m_w_corrected = m_w / (1 - beta1 ** t)
v_w_corrected = v_w / (1 - beta2 ** t)

# Mise à jour
weights -= learning_rate * m_w_corrected / (np.sqrt(v_w_corrected) + epsilon)
```

---

## TECHNIQUES D'OPTIMISATION

### Early Stopping

Arrêter l'entraînement quand la performance sur la validation ne s'améliore plus :

```python
best_val_loss = float('inf')
patience = 10
patience_counter = 0

for epoch in range(max_epochs):
    # ... entraînement ...
    
    val_loss = evaluate_on_validation(model, X_val, y_val)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        save_model(model)  # Sauvegarder le meilleur modèle
    else:
        patience_counter += 1
        
    if patience_counter >= patience:
        print(f"Early stopping à l'epoch {epoch}")
        break
```

### Régularisation

**L2 Regularization (Weight Decay) :**
```python
# Ajouter à la loss
l2_loss = lambda_reg * np.sum(weights ** 2)
total_loss = cross_entropy_loss + l2_loss

# Ajouter au gradient des poids
dLoss_dW += 2 * lambda_reg * weights
```

**Dropout (pour les couches cachées) :**
```python
# Pendant l'entraînement
if training:
    dropout_mask = np.random.binomial(1, keep_prob, size=activations.shape)
    activations *= dropout_mask / keep_prob
```

### Normalisation des données

```python
# Standardisation (mean=0, std=1)
X_normalized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

# Min-Max scaling (entre 0 et 1)
X_normalized = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))
```

---

## DEBUGGING ET MONITORING

### Vérifications importantes

1. **Gradient checking :** Vérifier que les gradients calculés sont corrects
2. **Loss diminue :** La loss doit globalement diminuer
3. **Pas d'explosion/disparition des gradients :** Surveiller les normes des gradients
4. **Overfitting :** Comparer train_loss vs val_loss

### Métriques à suivre

```python
# À chaque epoch
print(f"Epoch {epoch+1}/{epochs}")
print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
```

**Architecture de debugging recommandée :**
- Commencer avec un modèle simple (1-2 couches cachées)
- Vérifier que le modèle peut overfitter sur un petit subset
- Puis ajouter de la régularisation et complexité
